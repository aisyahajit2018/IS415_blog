---
title: "In-class Exercise 8"
description: |
  In this in-class exercise, I will learn how to perform geographical segmentation by using appropriate R packages. I will also use approriate R packages for performing cluster analysis and visualising clustering results.A short description of the post. This exercise is based on hands on exercise 8 but contains additional notes taken during class. 
author:
  - name: Nor Aisyah
    url: https://www.linkedin.com/in/nor-aisyah/
date: 10-11-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 4
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# 1. The Analytical Question

In geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using **multivariate** data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

# 2. Dataset

2 data sets will be used in this study:

- **Myanmar Township Boundary Data (i.e. myanmar_township_boundaries)** : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.
- **Shan-ICT.csv**: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.

Both data sets are download from Myanmar Information Management Unit (MIMU)

# 3. Install and Load Packages
```{r echo=TRUE, eval=TRUE}
packages = c('rgdal', 'spdep', 'tmap', 'sf', 'ggpubr', 'cluster', 'factoextra', 'NbClust', 'heatmaply', 'corrplot', 'psych', 'tidyverse')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
    }
  library(p,character.only = T)
}
```

Below are the functions we will be perform using the respective R packages:

- Spatial data handling: sf, rgdal and spdep
- Attribute data handling: tidyverse, especially readr, ggplot2 and dplyr
- Choropleth mapping: tmap
- Multivariate data visualisation and analysis: coorplot, ggpubr, and heatmaply
- Cluster analysis: cluster
  
# 4. Data Import and Preparation
## 4.1 Importing geospatial data into R environment
- The Myanmar Township Boundary GIS data is in ESRI shapefile format. 
- It will be imported into R environment by using the *st_read()* function of **sf** package.
- Here, we specifically filter out the 3 provinces East, North and South of Shan

```{r echo=TRUE, eval=TRUE}
shan_sf <- st_read(dsn = "data/geospatial", layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)"))
```


### 4.1.1 View newly created sf dataframe

```{r echo=TRUE, eval=TRUE}
shan_sf
```

### 4.1.2 Reveal the data type of fields 

- Since shan_sf is conformed to tidy framework, we can also *glimpse()* to reveal the data type of it’s fields.

```{r echo=TRUE, eval=TRUE}
glimpse(shan_sf)
```

## 4.2 Importing aspatial data into R environment

- Below, we use **read_csv()** of **readr** package to read CSV file (It will be saved in R’s * tibble data.frame* format)
- Use **summary()** to reveal the summary statistics of ict data.frame

```{r echo=TRUE, eval=TRUE}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
summary(ict)
```

- There are a total of 11 fields and 55 observation in the tibble data.frame.
- The smallest township has a total household of 3318 while the largest township has a total population of 82604.

## 4.3 Derive new variables using dplyr package

- The unit of measurement of the values are **no. of household**.
- Using these values directly will be **bias** by the underlying total number of households.
- In general, the **townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc**.
- In order to overcome this problem, we will **derive the penetration rate** of each ICT variable using code chunk below
- Then, we use **summary()** to reveal the summary statistics of ict_derived data.frame
- Here, we derive the proportion of owning the ICT over the total household then multiply by a constant which is 1000. 

```{r echo=TRUE, eval=TRUE}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
summary(ict_derived)
```

- From the results above, we can see that there are **6 new fields** that have been added into the data.frame:
  - 6 new fields are: RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR.
  
# 5. Exploratory Data Analysis (EDA)
## 5.1 EDA using statistical graphics

### 5.1.1 Histograms

- Plot the distribution of the variables (i.e. Number of households with radio) 
- Histogram is useful to identify the **overall distribution** of the data values (i.e. left skewed, right skewed or normal distribution)

```{r echo=TRUE, eval=TRUE}
ggplot(data=ict_derived, aes(x=`RADIO`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

### 5.1.2 Boxplot

- Boxplots are useful to detect if there are **outliers**.

```{r echo=TRUE, eval=TRUE}
ggplot(data=ict_derived, aes(x=`RADIO`)) +
  geom_boxplot(color="black", fill="light blue")
```

**My notes**: Observations that we can see from the histogram and boxplot: 

- We can see that both the histogram and boxplot above have a right skew. 
- The median radio penetration rate is now approximately 210 compared to the previous 25000. 
- One outlier with a value of approximately 490 is present.


### 5.1.3 Histograms of newly derived variables

- Plot the distribution of the newly derived variables (i.e. Radio penetration rate)

```{r echo=TRUE, eval=TRUE}
ggplot(data=ict_derived, aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

### 5.1.4 Boxplot of newly derived variables

```{r echo=TRUE, eval=TRUE}
ggplot(data=ict_derived, aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", fill="light blue")
```

*What can you observe from the distributions reveal in the histogram and boxplot?
**My notes**: The penetration rate of radio is more normally distributed compared to the no. of households owning radio.

### 5.1.4 Multiple Histograms of selected variables

- Multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame
- In the code chunk below, we will create the individual histograms then use **ggarange()** of **ggpubr** package to group the histograms together

```{r echo=TRUE, eval=TRUE}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

**My notes**: 

- Here we are performing univariate EDA so that we can compare the data range and see it is very large
- We can see that the penetration rates of computer, internet, llphone have an obvious right skew
- TV, Radio and Mphone penetration rates have a normally distributed values
- The range of these histograms also differs a bit, with `TV_PR` having the largest range while `COMPUTER_PR` has the smallest range
- The range of all histograms are less than 500


## 5.2 EDA using choropleth map
### 5.2.1 Joining geospatial data with aspatial data

- Before we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one
- This can be done using the **left_join** function of **dplyr** package
- *shan_sf* simple feature data.frame will be used as the base data object 
- *ict_derived* data.frame will be used as the join table.
- The unique identifier used to join both data objects is *TS_PCODE*

```{r echo=TRUE, eval=TRUE}
shan_sf <- left_join(shan_sf, ict_derived, by=c("TS_PCODE"="TS_PCODE"))
```

- It is important to note that there is **no new output data been created**. 
- Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.

### 5.2.2 Preparing a choropleth map

- Use *qtm()* function of **tmap** package to look at the distribution of Radio penetration rate of Shan State at township

```{r echo=TRUE, eval=TRUE}
qtm(shan_sf, "RADIO_PR")
```

### 5.2.3 Reveal distribution (bias)

- To reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create 2 choropleth maps 
  - one for the total number of households (TT_HOUSEHOLDS.map) and 
  - one for the total number of household with Radio (RADIO.map) 

```{r echo=TRUE, fig.width=10, fig.height=8}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 
RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 
tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

Results above show that:
 
-  Townships with **relatively larger number of households** are also showing **relatively higher number of radio ownership**.
- Therefore, it is not very appropriate to use no. of radio ownership to plot the map but rather derive the proportion.  
- We will do this in the next section:

### 5.2.4 Distribution of total number of households and Radio penetration rate

```{r echo=TRUE, fig.width=8, fig.height=8}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

*Can you identify the differences?*

- We can see that there are townships which have higher radio penetration rate compared to other townships which have higher total number of households.
  - So, this means that townships with relatively larger no. of households does not necessarily have relatively higher radio penetration rate.
- This is indicated by the darker orange shade in `RADIO_PR` but lighter yellow shade in `TT_HOUSEHOLDS.`
- The most obvious town that show this observation is the town at the top



## 5.3 Correlation Analysis

Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.

- Here, we use *corrplot.mixed()* of **corrplot** package to visualise and analyse the correlation of the input variables.
- There are 7 visualization methods (parameter method) in corrplot package, named 'circle', 'square', 'ellipse', 'number', 'shade', 'color', 'pie'

```{r echo=TRUE, fig.width=10, fig.height=8}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

Correlation plot above show that:

- **`COMPUTER_PR` and `INTERNET_PR`** are highly correlated.
- This suggest that only 1 of them should be used in the cluster analysis instead of both.


# 6. Hierarchy Cluster Analysis
## 6.1 Prepare variables
### 6.1.1 Extracting clustering variables

- Here, we extract the clustering variables from the shan_sf simple feature object into data.frame.
- Before we do that, we will also drop the geometry by setting it to NULL.
- head() function is then used to display the first 10 rows.

```{r echo=TRUE, eval=TRUE}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

- Here, we can see that final clustering variables list **does not include variable `INTERNET_PR`** because it is **highly correlated** with variable `COMPUTER_PR`.

### 6.1.2 Change row names

- Change the rows by township name instead of row number 

```{r echo=TRUE, eval=TRUE}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

- Here, you can see the row label have changed from number to the township names (**look at the left**)

### 6.1.3 Delete TS.x field

- After we have successfully set the row label to the township name, we can proceed with deleting the old township column

```{r echo=TRUE, eval=TRUE}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## 6.2 Data Standardisation

In general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to **avoid cluster analysis result to be biased due to the clustering variables with large values**, it is useful to **standardise** the input variables before performing cluster analysis.

There are 3 variable standardisation techniques as we have learnt in class:

- Min-Max
- Z-score
- Decimal scaling
  - Assume there are numbers which ranges from 90 to 150. To normalize by decimal scaling:
  - Find the largest number in the given range
  - Count the number of digits in the largest number (i.e., j = 3)
  - Divide each number by 10^j (i.e., 10^3 = 1000)
  - Therefore, the normalized valued for 90 and 150 is 0.09 and 0.15
  
In this exercise, we will only perform the first 2.

### 6.2.1 Min-Max standardisation

In the code chunk below we use:

- *normalize()* of **heatmaply** package to standardise the clustering variables by using Min-Max method.
- *summary()* is then used to display the summary statistics of the standardised clustering variables.

```{r echo=TRUE, eval=TRUE}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

Results above show that:

- Values range of the Min-max standardised clustering variables are now 0-1

### 6.2.2 Z-score standardisation

In the code chunk below we use:

- *scale()* of Base R to perform Z-score standardisation
- *describe()* of **psych** package is used here instead of summary() of Base R because the earlier provides standard deviation.

**Warning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.**

```{r echo=TRUE, eval=TRUE}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

Results above show that:

- The mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.

### 6.2.3 Visualising the standardised clustering variables

- It is a good practice to visualise their distribution graphically.
- Plot the scaled `Radio_PR` field

```{r echo=TRUE, eval=TRUE}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")
shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")
ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

- Overall distribution of the clustering variables will change after the data standardisation. 
- Hence, it is advisable **NOT** to perform data standardisation if the values range of the clustering variables are not very large.

## 6.3 Computing proximity matrix

- Here, we use *dist()* of R to calculate distance matrix then list the content of proxmat for visual inspection
- *dist()* supports 6 distance proximity calculations, they are: 
  - euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.
- In the code chunk below however, we use **euclidean** method.

```{r echo=TRUE, eval=TRUE}
proxmat <- dist(shan_ict, method = 'euclidean')
proxmat
```

## 6.4 Hierarchical clustering
### 6.4.1 Compute hierarchical clustering

- Here, we use *hclust()* of **R** stats 
 - The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.
- *hclust()* employed *agglomeration* method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

```{r echo=TRUE, eval=TRUE}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

### 6.4.2 Plot tree

- Use *plot()* of **R** Graphics to plot the tree
  - cex paramter is where we indicate the amount by which plotting text and symbols should be scaled relative to the default
  - 1=default, 1.5 is 50% larger, 0.5 is 50% smaller, etc.

```{r echo=TRUE, eval=TRUE}
plot(hclust_ward, cex = 0.6)
```

## 6.5 Select optimal clustering algorithm

One of the challenge in performing hierarchical clustering is to:

- Identify stronger clustering structures. 
- The issue can be solved by using use *agnes()* of **cluster** package. 
  - It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).
- In the code chunk below, we compute the agglomerative coefficients of all hierarchical clustering algorithms.
- We use a function to help us do this efficiently. *method = x"*

```{r echo=TRUE, eval=TRUE}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")


ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

Results above show that: 

- Ward’s method provides the strongest clustering structure among the 4 methods assessed.
- Hence, in the subsequent analysis,**only Ward’s method will be used**.

## 6.6 Determining Optimal Clusters

There are 3 commonly used methods to **determine the optimal clusters**, they are:

- Elbow Method
- Average Silhouette Method
- Gap Statistic Method : We will use this in this exercise

### 6.6.1 Gap Statistic Method

- The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data.
- The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). 
- This means that the clustering structure is far away from the random uniform distribution of points.

#### 6.6.1.1 Compute gap statistic 

- Use *clusGap()* of **cluster** package
- *hcut* function used is from **factoextra** package.

```{r echo=TRUE, eval=TRUE}
set.seed(12345)
gap_stat <- clusGap(shan_ict, FUN = hcut, nstart = 25, K.max = 10, B = 50)
print(gap_stat, method = "firstmax") # print result
```

#### 6.6.1.2 Visualise plot

- **fviz_gap_stat()** of **factoextra** package to visualise the plot

```{r echo=TRUE, eval=TRUE}
fviz_gap_stat(gap_stat)
```

Results above show that:

- Recommended no. of cluster to retain is 1. However, it is **not logical to retain only one cluster**. 
- By examining the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the **next best cluster to pick**.

**Note:** 

- In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides **30 indices** for determining the relevant number of clusters 
- and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

## 6.7 Interpreting the dendrograms

- In the dendrogram displayed above, each leaf corresponds to 1 observation. 
- As we move up the tree, observations that are **similar to each other** are **combined into branches**, which are themselves fused at a higher height.
- The **height** of the fusion, provided on the vertical axis, indicates the **(dis)similarity** between 2 observations.     - The **higher the height of the fusion**, the **less similar** the observations are.
- Note: conclusions about the **proximity** of 2 observations can be **drawn only based on the height where branches containing those two observations first are fused**. We cannot use the proximity of 2 observations along the horizontal axis as a criteria of their similarity.

In the code chunk below, we use:

- **rect.hclust()** of **R stats** to draw the dendrogram with a border around the selected clusters.
- We should plot first before using the rect.hclust function
- The argument **border** is used to specify the **border colors** for the rectangles.
  - For our 6 clusters, we indicate 2:5 because we want to randomly assign colours to the cluster border
  - The first and last cluster is automatically assigned the colour. 

```{r echo=TRUE, eval=TRUE}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, k = 6, border = 2:5)
```

- Cluster 4 is the biggest

## 6.8 Visually-driven hierarchical clustering analysis

With **heatmaply** package, we are able to build both highly interactive cluster heatmap or static cluster heatmap.

### 6.8.1 Transforming the data frame into a matrix

- The data was loaded into a data frame, but it has to be a data matrix to make your heatmap.
- Use *data.matrix()* of base R package

```{r echo=TRUE, eval=TRUE}
shan_ict_mat <- data.matrix(shan_ict)
```

### 6.8.2 Plotting interactive cluster heatmap using heatmaply()

- Use *heatmaply()* to build an interactive cluster heatmap.

```{r echo=TRUE, fig.width=10, fig.height=8}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Greens,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```



## 6.9 Mapping the clusters formed

With close examination of the dendragram above, we have decided to retain 6 clusters.

### 6.9.1 Derive 5-cluster model 

- Use *cutree()* of **R Base** to derive a 6-cluster model.
- cutree will return us numerical values so we need to use as.factor to make it categorical

```{r echo=TRUE, eval=TRUE}
groups <- as.factor(cutree(hclust_ward, k=6))
```

### 6.9.2 Append *groups*

To visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.

- the groups list object will be converted into a matrix;
- *cbind()* is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and
- *rename()* of **dplyr** package is used to rename as.matrix.groups field as CLUSTER.

```{r echo=TRUE, eval=TRUE}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

### 6.9.3 Plot choropleth map

-  *qtm()* of **tmap** package 

```{r echo=TRUE, eval=TRUE}
qtm(shan_sf_cluster, "CLUSTER")
```

Results above show that: 

- Clusters are very *fragmented*
- This is 1 of the major limitations when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.


# 7. Spatially Constrained Clustering - SKATER approach

Here, we will derive spatially constrained cluster by using SKATER method.

## 7.1 Convert into SpatialPolygonsDataFrame

- First, we need to convert shan_sf into SpatialPolygonDataFrame. 
  - This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.
- Then, we use *as_Spatial()* of **sf** package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.

```{r echo=TRUE, eval=TRUE}
shan_sp <- as_Spatial(shan_sf)
```

## 7.2 Neighbour List
### 7.2.1 Compute Neighbour List

- *poly2nd()* of **spdep** package is used to compute the neighbours list from polygon list.

```{r echo=TRUE, eval=TRUE}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

### 7.2.2 Plot the neighbours list
- Plot the neighbours list on shan_sp 
- Since we now can plot the community area boundaries as well, we plot this graph on top of the map

- The 1st plot command gives the **boundaries**.
- Then, the next command gives plot of the **neighbor list object**, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons.
- These are used as the nodes for the graph representation. 
- We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.

```{r echo=TRUE, fig.width=10, fig.height=8}
plot(shan_sp, border=grey(.5))
plot(shan.nb, coordinates(shan_sp), col="blue", add=TRUE)
```

## 7.3 Compute minimum spanning tree
### 7.3.1 Calculate edge costs

- *nbcosts()* of **spdep** package is used to compute the cost of each edge.
  - It is the distance between it nodes.
- This function compute this distance using a data.frame with observations vector in each node.

```{r echo=TRUE, eval=TRUE}
lcosts <- nbcosts(shan.nb, shan_ict)
```

- For each observation, this gives the **pairwise dissimilarity** between its values on the 5 variables and the values for the neighbouring observation (from the neighbour list). 
- Basically, this is the notion of a generalised weight for a spatial weights matrix.
- Next, we will incorporate these costs into a **weights object** in the same way as we did in the calculation of inverse of distance weights. 
- In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.

- Here, we use *nb2listw(*) of **spdep** package
- We specify the style as B to make sure the cost values are not row-standardised.

```{r echo=TRUE, eval=TRUE}
shan.w <- nb2listw(shan.nb, lcosts, style="B")
summary(shan.w)
```

### 7.3.2 Computing minimum spanning tree

- Minimum spanning tree is computed by mean of the **mstree()** of **spdep** package

```{r echo=TRUE, eval=TRUE}
shan.mst <- mstree(shan.w)
```

- After computing the MST, check its class and dimension by using the code chunk below.

```{r echo=TRUE, eval=TRUE}
class(shan.mst)
dim(shan.mst)
```

Results above show that:

- The dimension is **54 and not 55**. 
- This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.


- We then display the content of *shan.mst* by using *head()* 

```{r echo=TRUE, eval=TRUE}
head(shan.mst)
```

- Plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge.
- As before, we plot this together with the township boundaries. 
- We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.

```{r echo=TRUE, fig.width=10, fig.height=8}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, coordinates(shan_sp), 
     col="blue", cex.lab=0.7, cex.circles=0.005, add=TRUE)
```

## 7.4 Computing spatially constrained clusters using SKATER method

- use *skater()* of **spdep** package to compute the spatially constrained cluster
- *skater()* takes in 3 mandatory arguments:
  - the first 2 columns of the MST matrix (i.e. not the cost)
  - the data matrix (to update the costs as units are being grouped)
  - the no. of cuts. **Note:** It is set to one less than the no. of clusters. 
    - So, the value specified is not the no. of clusters, but the **no. of cuts** in the graph, one less than the number of clusters.
    
### 7.4.1 Compute the spatially constrained cluster
```{r echo=TRUE, eval=TRUE}
clust6 <- skater(shan.mst[,1:2], shan_ict, method = "euclidean", 5)
```

### 7.4.2 Examine result 

- Examine result of the *skater()* is an object of class skater contents

```{r echo=TRUE, eval=TRUE}
str(clust6)
```

- The most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). 
- This is followed by a detailed summary for each of the clusters in the edges.groups list. 
- Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.

### 7.4.3 Check the cluster assignment
```{r echo=TRUE, eval=TRUE}
ccs6 <- clust6$groups
ccs6
```

### 7.4.4 Find out how many observations are in each cluster 

- We can find out how many observations are in each cluster by means of the table command. 
- Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. 
- For example, the first list has node with dimension 12, which is also the no. of observations in the first cluster.

```{r echo=TRUE, eval=TRUE}
table(ccs6)
```

### 7.4.5 Plot the pruned tree

Lastly, we can also plot the pruned tree that shows the 5 clusters on top of the townshop area.

```{r echo=TRUE, fig.width=10, fig.height=8}
plot(shan_sp, border=gray(.5))
plot(clust6, coordinates(shan_sp), cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"), cex.circles=0.005, add=TRUE)
```

## 7.5 Visualising the clusters in choropleth map

- To plot the newly derived clusters by using SKATER method.

```{r echo=TRUE, fig.width=10, fig.height=8}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

For easy comparison, it will be better to place *both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other*.

```{r echo=TRUE, fig.width=10, fig.height=8}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 
shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 
tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```


