---
title: "Hands-on Exercise 9"
description: |
  In this hands on exercise, I learn how to calibrate geographically weighted regression models by using GWmodel package of R.
author:
  - name: Nor Aisyah
    url: https://www.linkedin.com/in/nor-aisyah/
date: 10-17-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 4
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



# 1. Background

Geographically weighted regression (GWR) is a spatial statistical technique that:

- takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and 
- models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).

We will learn:

- How to build hedonic pricing models by using GWR methods. 
- The dependent variable is the resale prices of condominium in 2015. 
- The independent variables are divided into either structural and locational.

# 2. Datasets

2 data sets will be used in this model building exercise, they are:

- URA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)
- condo_resale_2015 in csv format (i.e. condo_resale_2015.csv)

# 3. Import packages

The code chunks below installs and launches these R packages into R environment.

```{r echo=TRUE, eval=TRUE}
packages = c('olsrr', 'corrplot', 'ggpubr', 'sf', 'spdep', 'GWmodel', 'tmap', 'tidyverse')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```

More on the packages used: 

- Geospatial statistical modelling package: **GWmodel**
- Spatial data handling: **sf**
- Attribute data handling: **tidyverse**, especially **readr**, **ggplot2** and **dplyr**
- Choropleth mapping: **tmap**


**More on GWmodel package:**

- GWmodel package provides a collection of localised spatial statistical methods, namely: 
  - GW summary statistics, 
  - GW principal components analysis, 
  - GW discriminant analysis and various forms of GW regression; 
  - some of which are provided in basic and robust (outlier resistant) forms. 
- Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.


# 4 Geospatial Data Wrangling
## 4.1 Import geospatial data

- Here, we import *MP_SUBZONE_WEB_PL* shapefile by using *st_read()* of **sf** packages.
- Shapefile consists of URA Master Plan 2014’s planning subzone boundaries
- Polygon features are used to represent these geographic boundaries
- The GIS data is in svy21 projected coordinates systems

```{r echo=TRUE, eval=TRUE}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

Report above shows that:

- R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called *`mpsz`* and it is a simple feature object.
- The geometry type is multipolygon. 
- It is also important to note that mpsz simple feature object **does not have EPSG information**.


## 4.2 Update CRS information

- Update the newly imported mpsz with the correct ESPG code (i.e. 3414)
- Verify newly transformed mpsz_svy21.

```{r echo=TRUE, eval=TRUE}
mpsz_svy21 <- st_transform(mpsz, 3414)
st_crs(mpsz_svy21)
```

Results above show that:

- The EPSG: is indicated as 3414 now.


## 4.3 Reveal the extent of mpsz_svy21

- Here, we reveal the extent of mpsz_svy21 using *st_bbox()* of **sf** package

```{r echo=TRUE, eval=TRUE}
st_bbox(mpsz_svy21)
```


# 5 Aspatial Data Wrangling
## 5.1 Import the aspatial data

Here, we use:
-  *read_csv()* function of **readr** package to import condo_resale_2015 into R as a tibble data frame called condo_resale.
-  *glimpse()* to display the data structure.

```{r echo=TRUE, eval=TRUE}
condo_resale <- read_csv("data/aspatial/Condo_resale_2015.csv")
glimpse(condo_resale)
```

- See summary statsitics of *condo_resale*

```{r echo=TRUE, eval=TRUE}
summary(condo_resale)
```


## 5.2 Convert aspatial data frame into a sf object

Here, we use: 

- *st_as_sf()* of sf package to convert aspatial data frame to sf object and
- *st_transform()* of **sf** package to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).

```{r echo=TRUE, eval=TRUE}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
head(condo_resale.sf)
```



# 6 Exploratory Data Analysis
## 6.1 EDA using statistical graphics
### 6.1.1 Plot distribution

- Plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) 

```{r echo=TRUE, eval=TRUE}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Results above reveals: 

- A right skewed distribution. 
- This means that more condominium units were transacted at relative lower prices.
- Statistically, the skewed distribution can be normalised by using log transformation which we will be doing in the next section.

### 6.1.2 Normalise using Log Transformation

Here, we will:

- Derive a new variable called **`LOG_SELLING_PRICE`** by using a log transformation on the variable SELLING_PRICE.
- It is performed using *mutate()* of **dplyr** package.

```{r echo=TRUE, eval=TRUE}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

### 6.1.3 Plot Histogram of Count by **LOG_SELLING_PRICE** 

```{r echo=TRUE, eval=TRUE}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Notice that the distribution is relatively **less skewed after the transformation**.


## 6.2 Multiple Histogram Plots distribution of variables

Here, we will: 

- First create 12 histograms. Then,
- Use *ggarrnage()* of **ggpubr** package to organise these histogram into a 3 columns by 4 rows small multiple plot.

```{r echo=TRUE, eval=TRUE}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")  
PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT, PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  ncol = 3, nrow = 4)
```

## 6.3 Drawing Statistical Point Map

Here, we will reveal the geospatial distribution condominium resale prices in Singapore. 

- The map will be prepared by using tmap package.
  - *tmap_mode("view")* to use the interactive mode of tmap
- Then, create an interactive point symbol map
  - *tm_dots()* is used instead of *tm_bubbles()*
  - set.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.
- Lastly, *tmap_mode("plot")* to display plot mode

**Note: Display as plot mode for now**

```{r echo=TRUE, eval=TRUE}
tmap_mode("plot")
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```


# 7 Hedonic Pricing Modelling in R
## 7.1 Simple Linear Regression Method
### 7.1.1 Build Simple Linear Regression model

- Build a simple linear regression model by using:
  - `SELLING_PRICE` as the dependent variable and 
  - `AREA_SQM` as the independent variable.
- *lm()* returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).
- *summary()* and *anova()* can be used to obtain and print a summary and analysis of variance table of the results.
- The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by **lm**.

```{r echo=TRUE, eval=TRUE}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
summary(condo.slr)
```

Results above show that:

- `SELLING_PRICE` can be explained by using the formula:

      *y = -258121.1 + 14719x1*

- *R-squared* of **0.4518** reveals that the simple regression model built is able to explain about 45% of the resale prices.
- Since p-value is much smaller than 0.0001, we will **reject the null hypothesis** that mean is a good estimator of `SELLING_PRICE`.
- This will allow us to infer that simple linear regression model above is a **good estimator** of `SELLING_PRICE`


- The Coefficients: section of the report reveals that the p-values of both the estimates of the **`Intercept`** and **`ARA_SQM`** are **smaller than 0.001**.
- In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. 
- As a result, we will be able to infer that the B0 and B1 are good parameter estimates.

### 7.1.2 Visualise best fit curve

Here, we visualise the best fit curve on a scatterplot: 

- Using *lm()* as a method function in **ggplot’s** geometry

```{r echo=TRUE, eval=TRUE}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

Figure above reveals that there are a few statistical **outliers** with relatively high selling prices.


## 7.2 Multiple Linear Regression Method
### 7.2.1 Visualise relationships of independent variables

- It is important to ensure that the independent variables used are not highly correlated to each other. 
- If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. 
- This phenomenon is known as **multicollinearity** in statistics.

**Correlation matrix** is commonly used to visualise the relationships between the independent variables. 

- Beside the *pairs()* of R, there are many packages that support the display of a correlation matrix. 
- In this section, the **corrplot** package will be used.
- To plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.
- **Matrix reorder** is very important for mining the hidden structure and pattern in the matrix. 
- There are 4 methods in corrplot (parameter order), 
  - namely “**AOE**”, “**FPC**”, “**hclust**”, “**alphabet**”. 
- Alphabet order is used to order the variables alphabetically.

```{r echo=TRUE, eval=TRUE, fig.width=8, fig.height=8}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

Results above show that:

- **`Freehold`** is highly correlated to **`LEASE_99YEAR`**. 
- Thus, it is wiser to only include either one of them in the subsequent model building. 
- As a result, **`LEASE_99YEAR`** is excluded in the subsequent model building.

### 7.2.2 Build a hedonic pricing model using multiple linear regression method
#### 7.2.2.1 Calibrate the multiple linear regression model

- Use *lm()* to calibrate the multiple linear regression model.
 
```{r echo=TRUE, eval=TRUE}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET  + PROX_KINDERGARTEN  + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_SUPERMARKET + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf)
summary(condo.mlr)
```

Results above show that:

- **Not all** the independent variables are statistically significant. 
- We will revise the model by removing those variables which are not statistically significant.

#### 7.2.2.2 Calibrate the revised model 
```{r echo=TRUE, eval=TRUE}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf)
ols_regress(condo.mlr1)
```

### 7.2.3 Check for multicolinearity

When performing OLS regression, we can use: 

-  **olsrr** provides a collection of very useful methods for building better multiple linear regression models:
  - comprehensive regression output
  - residual diagnostics
  - measures of influence
  - heteroskedasticity tests
  - collinearity diagnostics
  - model fit assessment
  - variable contribution assessment
  - variable selection procedures
- the *ols_vif_tol()* of **olsrr** package is used to test if there are signs of multicollinearity.

```{r echo=TRUE, eval=TRUE}
ols_vif_tol(condo.mlr1)
```

Results above show that:

- There are no signs of multicollinearity among the independent variables as the VIF of the independent variables are less than 10.


### 7.2.4 Test for Non-Linearity

- In multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.
- Here, we use *ols_plot_resid_fit()* of **olsrr** package to perform linearity assumption test.

```{r echo=TRUE, eval=TRUE}
ols_plot_resid_fit(condo.mlr1)
```

Results above show that:

- Most of the data points are scattered around the 0 line.
- Hence we can safely conclude that the relationships between the dependent variable and independent variables are **linear.**

### 7.2.5 Test for Normality Assumption

- Use *ols_plot_resid_hist()* of **olsrr** package to perform normality assumption test.

```{r echo=TRUE, eval=TRUE}
ols_plot_resid_hist(condo.mlr1)
```

Results above show that:

- Reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.

For formal statistical test methods, the *ols_test_normality()* of **olsrr** package can be used as well, 

```{r echo=TRUE, eval=TRUE}
ols_test_normality(condo.mlr1)
```

Results above show that:

- p-values of the four tests are way smaller than the alpha value of 0.05. 
- Hence we will reject the null hypothesis that the residual **does NOT resemble normal distribution**.

### 7.2.6 Test for Spatial Autocorrelation

- The hedonic model we try to build are using geographically referenced attributes.
- Hence it is also important for us to visual the residual of the hedonic pricing model.
- In order to perform spatial autocorrelation test, we need to convert **condo_resale.sf** simple into a SpatialPointsDataFrame.

#### 7.2.6.1 Export residual of hedonic pricing model

- Export the residual of the hedonic pricing model and save it as a data frame.

```{r echo=TRUE, eval=TRUE}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

#### 7.2.6.2 Join with **condo_resale.sf** object

-  Join the newly created data frame with condo_resale.sf object.

```{r echo=TRUE, eval=TRUE}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

#### 7.2.6.3 Convert to SpatialPointsDataFrame 

- Convert **condo_resale.res.sf** simple feature object into a SpatialPointsDataFrame because **spdep** package can only process sp conformed spatial data objects

```{r echo=TRUE, eval=TRUE}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

#### 7.2.6.4 Display interactive point symbol map

**Note: it is currently in plot mode**

```{r echo=TRUE, eval=TRUE}
tmap_mode("plot")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

Results above show that:

- There is signs of spatial autocorrelation.
- To prove that our observation is indeed true, the Moran’s I test will be performed

## 7.3 Moran’s I test

### 7.3.1 Compute the distance-based weight matrix

-  Compute the distance-based weight matrix by using *dnearneigh()* of **spdep** package

```{r echo=TRUE, eval=TRUE}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

### 7.3.2 Convert to a spatial weights

- *nb2listw()* of **spdep** package will be used to convert the output neighbours lists (i.e. nb) into a spatial weights

```{r echo=TRUE, eval=TRUE}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

### 7.3.3 Perform  Moran’s I test for residual spatial autocorrelation

- Use *lm.morantest()* of **spdep** package 

```{r echo=TRUE, eval=TRUE}
lm.morantest(condo.mlr1, nb_lw)
```

Results above show that:

- p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. 
- Hence, we will reject the null hypothesis that the residuals are randomly distributed.
- Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.


# 8. Building Hedonic Pricing Models using GWmodel

In this section, we will learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes.

## 8.1 Build Fixed Bandwidth GWR Model
### 8.1.1 Compute fixed bandwith

- *bw.gwr()* of **GWModel** package is used to determine the optimal fixed bandwidth to use in the model. 
  - Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.
- There are 2 possible approaches to determine the stopping rule, they are: 
  - **CV cross-validation approach** and 
  - **AIC corrected (AICc) approach**. 
    - We define the stopping rule using approach argument.

```{r echo=TRUE, eval=TRUE}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach="CV", kernel="gaussian", adaptive=FALSE, longlat=FALSE)
```

Results above show that:

- The recommended bandwidth is 971.3398 metres.

Quiz: Why is it in metres?

- The projection coordinated system is SVY21 which is in metres. That's why the results is showing in metres.

### 8.1.2 GWModel method - fixed bandwith

- To calibrate the gwr model using fixed bandwidth and gaussian kernel.
- The output is saved in a list of class "gwrm".
- We then display the model output

```{r echo=TRUE, eval=TRUE}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, bw=bw.fixed, kernel = 'gaussian', longlat = FALSE)
gwr.fixed
```

Results above show that:

- The **adjusted r-square of the gwr** is 0.8430418 which is **significantly better** than the **global multiple linear regression** model of 0.6472.

## 8.2 Build Adaptive Bandwidth GWR Model

Calibrate the gwr-based hedonic pricing model by using **adaptive** bandwidth approach.

### 8.2.1 Compute the adaptive bandwidth

- Similar to the earlier section, we will first use bw.ger() to determine the recommended data point to use.
- Note: adaptive argument set to TRUE.

```{r echo=TRUE, eval=TRUE}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach="CV", kernel="gaussian",
adaptive=TRUE, longlat=FALSE)
```

Results above show that:

- **30** is the **recommended data points** to be used

### 8.2.2 Construct the adaptive bandwidth gwr model

- Calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel 
- Then display the model output

```{r echo=TRUE, eval=TRUE}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, bw=bw.adaptive, kernel = 'gaussian', adaptive=TRUE, longlat = FALSE)
gwr.adaptive
```

Results above show that:

- The **adjusted r-square of the gwr** is **0.8561185** which is **significantly better** than the **global multiple linear regression** model of **0.6472**

# 9. Visualising GWR Output

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:

- **Condition Number**: evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers **larger than 30**, may be **unreliable**

- **Local R2**: these values **range between 0.0 and 1.0** and indicate **how well the local regression model fits observed y values**. 
  - **Very low values** indicate the local model is performing **poorly**. 
  - Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

- **Predicted**: estimated (or fitted) y values computed by GWR.
- **Residuals**: to obtain the residual values, the fitted y values are subtracted from the observed y values.     
  - Standardized residuals have a mean of zero and a standard deviation of 1. 
  - A cold-to-hot rendered map of standardized residuals can be produce by using these values.
- **Coefficient Standard Error**: these values measure the reliability of each coefficient estimate. 
  - **Confidence** in those estimates are **higher** when **standard errors are small** in relation to the actual coefficient values.   
  - **Large standard errors** may indicate **problems with local collinearity**.

They are all stored in a **SpatialPointsDataFrame** or **SpatialPolygonsDataFrame** object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called **SDF** of the output list.

## 9.1 Converting SDF into sf data.frame

- To visualise the fields in **SDF**, we need to first covert it into **sf** data.frame

```{r echo=TRUE, eval=TRUE}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r echo=TRUE, eval=TRUE}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r echo=TRUE, eval=TRUE}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

```{r echo=TRUE, eval=TRUE}
glimpse(condo_resale.sf.adaptive)
```


```{r echo=TRUE, eval=TRUE}
summary(gwr.adaptive$SDF$yhat)
```

## 9.2 Visualising local R2

- To create an interactive point symbol map
- **Note: currently, it is in plot mode**

```{r echo=TRUE, fig.width=10, fig.height=8}
tmap_mode("plot")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

## 9.3 By URA Planning Region
```{r echo=TRUE, fig.width=10, fig.height=8}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

